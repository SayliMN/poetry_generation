{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3f1b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ad8de6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee00e8b",
   "metadata": {},
   "source": [
    "### 1. load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f686b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# line: Two roads diverged in a yellow wood,\n",
    "# input_line: <sos> Two roads diverged in a yellow wood,\n",
    "# target_line: Two roads diverged in a yellow wood, <eos>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c275725",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "file_path = os.getcwd() + \"/data/robert_frost.txt\"\n",
    "\n",
    "for line in open(file_path):\n",
    "    line = line.rstrip()\n",
    "#     print(line)\n",
    "    if not line:\n",
    "        continue\n",
    "    \n",
    "    input_line = '<sos> ' + line # prepending start of string in input\n",
    "    target_line = line + ' <eos>' # apending end of string in target\n",
    "    \n",
    "    input_texts.append(input_line)\n",
    "    target_texts.append(target_line)\n",
    "    \n",
    "all_lines = input_texts + target_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06fc7e91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2872"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacacf64",
   "metadata": {},
   "source": [
    "### 2. Convert the lines/strings into arrays of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d40e0e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02ad4fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 104, 537, 538, 9, 7, 539, 540], [1, 5, 541, 6, 65, 31, 934, 141], [1, 5, 27, 24, 935, 152, 6, 221], [1, 5, 167, 67, 24, 17, 128, 17, 6, 65], [1, 4, 40, 11, 936, 9, 3, 937]]\n",
      "[[104, 537, 538, 9, 7, 539, 540, 2], [5, 541, 6, 65, 31, 934, 141, 2], [5, 27, 24, 935, 152, 6, 221, 2], [5, 167, 67, 24, 17, 128, 17, 6, 65, 2], [4, 40, 11, 936, 9, 3, 937, 2]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = MAX_VOCAB_SIZE, filters= '')\n",
    "tokenizer.fit_on_texts(all_lines)\n",
    "\n",
    "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
    "target_sequences = tokenizer.texts_to_sequences(target_texts)\n",
    "print(input_sequences[:5])\n",
    "print(target_sequences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0704b9c3",
   "metadata": {},
   "source": [
    "### 3. Find max sequence length in input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "389241ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 12\n"
     ]
    }
   ],
   "source": [
    "max_sequence_length_from_data = max(len(s) for s in input_sequences)\n",
    "print('Max sequence length:', max_sequence_length_from_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a608e9",
   "metadata": {},
   "source": [
    "### 4. Get word to integer mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd1da494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3056 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word2idx = tokenizer.word_index\n",
    "print('Found {} unique tokens.' .format(len(word2idx)))\n",
    "assert('<sos>' in word2idx)\n",
    "assert('<eos>' in word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1165d6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<sos>', 1),\n",
       " ('<eos>', 2),\n",
       " ('the', 3),\n",
       " ('to', 4),\n",
       " ('and', 5),\n",
       " ('i', 6),\n",
       " ('a', 7),\n",
       " ('of', 8),\n",
       " ('in', 9),\n",
       " ('you', 10)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 10 words to integer mapping\n",
    "from itertools import islice\n",
    "\n",
    "list(islice(word2idx.items(), 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e32bfc",
   "metadata": {},
   "source": [
    "### 5. Pad sequences to get a N x T matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72d2591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e24c51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (1436, 12)\n"
     ]
    }
   ],
   "source": [
    "max_sequence_length = min(max_sequence_length_from_data, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='post')\n",
    "target_sequences = pad_sequences(target_sequences, maxlen=max_sequence_length, padding='post')\n",
    "print('Shape of data tensor:', input_sequences.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29142268",
   "metadata": {},
   "source": [
    "### 6. load in pre-trained word vectors with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0557bf1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "\n",
    "# path_glove = os.getcwd() + \"/glove/glove.6B.50d.txt\"\n",
    "base_dir = os.getcwd() + \"/glove/\"\n",
    "filename = 'glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5e25cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "word2vec = {}\n",
    "with open(os.path.join(base_dir, filename)) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "    print('Found {} word vectors.' .format(len(word2vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e87d9",
   "metadata": {},
   "source": [
    "### 7. prepare embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed2ae9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n"
     ]
    }
   ],
   "source": [
    "print('Filling pre-trained embeddings...')\n",
    "num_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx.items():\n",
    "    if i < MAX_VOCAB_SIZE:\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7f96a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 50)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bdd157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff0ea7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
