{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b881469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c13d6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5226868d",
   "metadata": {},
   "source": [
    "### 1. load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62dcf0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# line: Two roads diverged in a yellow wood,\n",
    "# input_line: <sos> Two roads diverged in a yellow wood,\n",
    "# target_line: Two roads diverged in a yellow wood, <eos>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "323deb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "file_path = os.getcwd() + \"/data/robert_frost.txt\"\n",
    "\n",
    "for line in open(file_path):\n",
    "    line = line.rstrip()\n",
    "#     print(line)\n",
    "    if not line:\n",
    "        continue\n",
    "    \n",
    "    input_line = '<sos> ' + line # prepending start of string in input\n",
    "    target_line = line + ' <eos>' # apending end of string in target\n",
    "    \n",
    "    input_texts.append(input_line)\n",
    "    target_texts.append(target_line)\n",
    "    \n",
    "all_lines = input_texts + target_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34564776",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2872"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2876c85e",
   "metadata": {},
   "source": [
    "### 2. Convert the lines/strings into arrays of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecb3d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8895266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 104, 537, 538, 9, 7, 539, 540], [1, 5, 541, 6, 65, 31, 934, 141], [1, 5, 27, 24, 935, 152, 6, 221], [1, 5, 167, 67, 24, 17, 128, 17, 6, 65], [1, 4, 40, 11, 936, 9, 3, 937]]\n",
      "[[104, 537, 538, 9, 7, 539, 540, 2], [5, 541, 6, 65, 31, 934, 141, 2], [5, 27, 24, 935, 152, 6, 221, 2], [5, 167, 67, 24, 17, 128, 17, 6, 65, 2], [4, 40, 11, 936, 9, 3, 937, 2]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = MAX_VOCAB_SIZE, filters= '')\n",
    "tokenizer.fit_on_texts(all_lines)\n",
    "\n",
    "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
    "target_sequences = tokenizer.texts_to_sequences(target_texts)\n",
    "print(input_sequences[:5])\n",
    "print(target_sequences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fc9f5d",
   "metadata": {},
   "source": [
    "### 3. Find max sequence length in input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21c5d8f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 12\n"
     ]
    }
   ],
   "source": [
    "max_sequence_length_from_data = max(len(s) for s in input_sequences)\n",
    "print('Max sequence length:', max_sequence_length_from_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72aac9d",
   "metadata": {},
   "source": [
    "### 4. Get word to integer mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79abe96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3056 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word2idx = tokenizer.word_index\n",
    "print('Found {} unique tokens.' .format(len(word2idx)))\n",
    "assert('<sos>' in word2idx)\n",
    "assert('<eos>' in word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "602855ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<sos>', 1),\n",
       " ('<eos>', 2),\n",
       " ('the', 3),\n",
       " ('to', 4),\n",
       " ('and', 5),\n",
       " ('i', 6),\n",
       " ('a', 7),\n",
       " ('of', 8),\n",
       " ('in', 9),\n",
       " ('you', 10)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 10 word to integer mapping\n",
    "from itertools import islice\n",
    "\n",
    "list(islice(word2idx.items(), 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f74cee9",
   "metadata": {},
   "source": [
    "### 5. Pad sequences to get a N x T matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74fb9497",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82a22a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (1436, 12)\n"
     ]
    }
   ],
   "source": [
    "max_sequence_length = min(max_sequence_length_from_data, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='post')\n",
    "target_sequences = pad_sequences(target_sequences, maxlen=max_sequence_length, padding='post')\n",
    "print('Shape of data tensor:', input_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d691fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a7fc08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
